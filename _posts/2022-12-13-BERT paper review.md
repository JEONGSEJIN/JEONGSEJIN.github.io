---
title: BERT paper review
author:
  name: Sejin Jeong
  link: https://github.com/JEONGSEJIN
date: YYYY-MM-DD HH:MM:SS +09:00
categories: [NLP] # 공부하고, subtag 추가하기!
tags: [NLP, BERT]
---

Paper name  
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
![bert1](/assets/img/BERT_paper_review/slide1.JPG)

* * *

![bert2](/assets/img/BERT_paper_review/slide2.JPG)

* * *

![bert3](/assets/img/BERT_paper_review/slide3.JPG)

* * *

![bert4](/assets/img/BERT_paper_review/slide4.JPG)

* * *

![bert5](/assets/img/BERT_paper_review/slide5.JPG)

* * *

![bert6](/assets/img/BERT_paper_review/slide6.JPG)

* * *

![bert7](/assets/img/BERT_paper_review/slide7.JPG)

* * *

![bert8](/assets/img/BERT_paper_review/slide8.JPG)

* * *

![bert9](/assets/img/BERT_paper_review/slide9.JPG)

* * *

![bert10](/assets/img/BERT_paper_review/slide10.JPG)

* * *

![bert11](/assets/img/BERT_paper_review/slide11.JPG)

* * *

![bert12](/assets/img/BERT_paper_review/slide12.JPG)

* * *

![bert13](/assets/img/BERT_paper_review/slide13.JPG)

* * *

![bert14](/assets/img/BERT_paper_review/slide14.JPG)

* * *

![bert15](/assets/img/BERT_paper_review/slide15.JPG)

* * *

![bert16](/assets/img/BERT_paper_review/slide16.JPG)

* * *

![bert17](/assets/img/BERT_paper_review/slide17.JPG)

* * *

![bert18](/assets/img/BERT_paper_review/slide18.JPG)

* * *

![bert19](/assets/img/BERT_paper_review/slide19.JPG)

* * *

![bert20](/assets/img/BERT_paper_review/slide20.JPG)

* * *

![bert21](/assets/img/BERT_paper_review/slide21.JPG)

* * *

![bert22](/assets/img/BERT_paper_review/slide22.JPG)

* * *

![bert23](/assets/img/BERT_paper_review/slide23.JPG)

* * *

![bert24](/assets/img/BERT_paper_review/slide24.JPG)

* * *

![bert25](/assets/img/BERT_paper_review/slide25.JPG)

References  
- https://arxiv.org/abs/1810.04805
- https://youtu.be/IwtexRHoWG0
- https://youtu.be/moCNw4j2Fkw

Study Footprint
- https://drive.google.com/drive/folders/1zfy6Hgv5bTuUwi6PHwtd0Dee_eZQ296X?usp=sharing